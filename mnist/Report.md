## 基于全连接神经网络的手写数字识别模型的实现


Link: https://github.com/sydney0zq/nn-c



### 项目概述

本项目使用 C 语言在 MNIST 数据集上实现手写数字识别, 其典型应用在数字信封上的邮编地址填写的自动化。在机器学习如此盛行的今天, 大多数人使用 python 去实现这一任务, 使用 C 语言可以对网络的整个运作过程更为熟悉, 主要深化在对于 BP 算法的理解和整个过程的细节。

基于对 MNIST 数据集的了解, 我认为一个三层的全连接网络足以相当不错的完成此分类任务, 虽然 CNN 网络提取的特征更为有效, 但是实现起来有些复杂, 尤其是 BP 算法。那么整个模型叙述如下: 有三层网络, 第一层是图像的所有 pixel, 将图片压成一个向量输入, 之后中间是隐藏层, 作为实现复杂函数的必要层, 最后一层的节点数是数字的个数, 即 10 个 neuron, 输出是每一个数字的可能性(即概率)。下图就是本模型的一个示意图, 其具体定义在 `init_layer.c` 文件中。

![](http://okye062gb.bkt.clouddn.com/2017-08-07-141724.jpg)



我们将每一张图片向量化之后, 再输入到网络中, 那么每一层网络就通过权重加和操作, 再通过激活函数, 输出到下一层。当然一开始我们是直接使用了高斯标准分布函数进行初始化权重, 实践证明这比一开始都是 0 的情况收敛的要更快。就这样一层一层的向前传播并得到这个图片是什么值, 那么我们就需要把它真实的标签找出来, 计算一个损失值 loss, 之后再对 loss 进行求导 (对 W 和 b 进行求导), 求得的导数值再乘以学习率(learning rate) 再叠加到原来的 W 和 b 上, 就这样一层层的反向传播回去。当然我们的导数目标是让 loss 值变小, 这里计算 loss 的方法是交叉熵损失, 这在信息论也有讲述, 训练的 optimizer 是 SGD(Stochastic Gradient Descent)。就这样, 我们的目标就是让 loss 降的足够低, 当然也不能太低, 也要考虑过拟合问题。



### 项目难点

本项目难点其实不多, 我在这里大致说一些比较大的点。

首先是有原始数据集的压缩文件进行解压缩, 这部份是使用了 python 来写入到文件中, 然后利用操作系统的文件 API 进行读取, 我认为这样是比较方便的, C 语言直接读取图片是比较冗余和没有必要的。当然这样做的代价是我的训练数据集变大, 从 10 多 M 变成了几百 M, 不过这样也不算大, 感觉是值得的, 况且读取速度也很快。典型的空间换时间。

第二就是实现各种矩阵操作, 由于 C 语言没有 python 中的 numpy 库, 所以基本都是要靠自己去写一个个函数的, C++ 此类面向对象的语言写这种对象的可能要好一些, 但是 C 语言最大的好处是它足够的简单, 只要实现得当, 效果也并不差。在矩阵操作中, 有诸如矩阵加常数, 矩阵点乘等等操作, 总之实现起来相当麻烦, 但是通过模块调试基本一个下午可以实现所有的基本函数库。

第三就是 BP 算法, 还好这个网络较为简单, 反向传播算法并不难写。由于 C 语言是面向过程的, 那么这就意味着假如有 100 层的网络, 我就需要写最起码 100 多次, 每一步都需要指明。这也暴露了 C 语言的劣势所在。不过 C 语言底层特性带来的巨大效率优势也是值得考虑的。

第四就是内存泄漏, 这是个非常头疼的问题, 必须要解决掉。在这个上面我画的时间不多, 但是我没有找到比较漂亮的方法去解决内存泄漏的问题, 我用的方式是简单粗暴的, 在哪里使用了 `malloc`, 那么就标记, 要释放。虽然最后也保证了每一个堆的申请内存都被释放了, 但是感觉这种方法也是无奈之举, 这也算是 C 语言的一个比较麻烦的地方。

本项目由于较小, 我大概使用了一天的时间去写函数库和主函数, 一天的时间去 debug, 一个早上去修改超参数。总体上难度不大。



### 心得体会

C 语言总结来看, 最大的劣势就是面向过程, 这导致了层数不能任意拓展, 也导致了 BP 算法复杂度的增加, 但是其灵活的底层特性和高效率又是需要考虑的。有了面向对象的思想的话, 一个 neuron 怎么设计和一层怎么设计感觉也是比较难的问题。

我认为这种网络适合定制性较强的应用, 比如嵌入式方面的小系统。以后也可以用于将网络进行精简, 加速提升; 同时也适合需要对底层进行操作的应用, 比如对训练好的网络进行裁剪(ThiNet)。

当然因为之前对这种 FC 网络太熟悉了, 写起来更为顺畅。调超参数也调节了几次就达到了 95% 的准确率, 训练的时间也大概只用了几分钟。这得益于训练数据集较小也模型也很简单, 但在 SVHN 数据集上可能就有些困难。大概用 CNN 网络才容易达到好的效果。

不过这个小项目为我在深入 Machine Learning 打下了基础, 我可以更为轻松的上手 caffe 框架或者其他的底层框架而不用太吃力。总体来说是很有帮助的一个项目。
